---
title: "R Notebook"
output: html_notebook
---

```{r}
library(tidyverse)
library(splines)
th <- theme_minimal() + 
  theme(
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "#f7f7f7"),
    panel.border = element_rect(fill = NA, color = "#0c0c0c", size = 0.6),
    axis.text = element_text(size = 11),
    axis.title = element_text(size = 13),
    legend.position = "bottom"
  )
theme_set(th)
set.seed(1234)
```

```{r}
mean_fun <- function(times, peak_loc, height, end) {
  mu <- rep(0, length(times))
  rise <- peak_loc > times & times >= 0
  mu[rise] <- seq(0, height, length.out = sum(rise))
  fall <- times >= peak_loc & times < end
  mu[fall] <- seq(height, 0, length.out = sum(fall))
  approxfun(times, mu)
}

weighted_sampling <- function(N, intervals, weights) {
  samples <- list()
  interval_ix <- rmultinom(1, N, weights)
  for (i in seq_along(interval_ix)) {
    samples[[i]] <- runif(interval_ix[i], intervals[i, 1], intervals[i, 2])
  }
   
  unlist(samples)  
}

observations <- function(mean_fun, x, sigma = 1) {
  mean_fun(x) + rnorm(length(x), 0, sigma)
}

times <- seq(-10, 10, length.out = 200)
mf <- mean_fun(times, 1, 10, 5)
plot(times, mf(times), type = "l")
u <- runif(20, -10, 10)
points(u, observations(mf, u))
```

```{r}
u <- seq(-10, 10, length.out = 10)
intervals <- cbind(head(u, -1), tail(u, -1))
weights <- 5:1
weights <- weights / sum(weights)
locs <- weighted_sampling(1e4, intervals, weights)
hist(locs)
```

* We now have a way of simulating parameterized mean functions and observations with noise around them
* We also have ways of sampling locations to observe the function, based on a
weighted combination of uniforms
* We need a way of estimating these mean functions and evaluating the resulting
fit.
* We also need to evaluate many combinations of weightings, either using random
search or bayesian optimization

First, we attempt to estimate these mean functions using simple piecewise linear splines

```{r}
w <- c(1, 1, 1, 1, 3, 4, 3, 1, 1)
w <- w / sum(w)

u <- weighted_sampling(40, intervals, w)
y <- observations(mf, u)
fit <- lm(y ~ bs(u, df = 6, degree = 1, Boundary.knots = c(-10, 10)))
plot(times, mf(times), type = "l")
points(u, y, col = "red")
points(u, predict(fit), col = "blue")
f_hat <- predict(fit, newdata = data.frame(u = times))
points(times, f_hat, col = "blue")
```

```{r}
B <- 1e3
mse <- matrix(nrow = B, ncol = 2)

estimate_mse <- function(u, times, mf) {
  y <- observations(mf, u)
  fit <- lm(y ~ bs(u, df = 6, degree = 1, Boundary.knots = c(-10, 10)))
  f_hat <- predict(fit, newdata = data.frame(u = times))
  mean((f_hat - mf(times)) ^ 2)
}


for (b in seq_len(B)) {
  # random u's
  u <- runif(50, -10, 10)
  mse[b, 1] <- estimate_mse(u, times, mf)
  u <- weighted_sampling(50, intervals, w)
  mse[b, 2] <- estimate_mse(u, times, mf)
}

colnames(mse) <- c("uniform", "weighted")
mse <- as_tibble(mse) %>%
  mutate(id = 1:n()) %>%
  pivot_longer(-id, names_to = "sampling", values_to = "mse")

ggplot(mse) +
  geom_histogram(
    aes(mse, fill = sampling), 
    alpha = 0.8, position = "identity",
    bins = 50
  ) +
  scale_y_continuous(expand = c(0, 0, 0.1, 0)) +
  scale_x_continuous(expand = c(0, 0))
```
